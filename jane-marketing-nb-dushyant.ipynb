{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.15","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84493,"databundleVersionId":9871156,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-22T00:57:21.150204Z","iopub.execute_input":"2024-10-22T00:57:21.150599Z","iopub.status.idle":"2024-10-22T00:57:23.029822Z","shell.execute_reply.started":"2024-10-22T00:57:21.150566Z","shell.execute_reply":"2024-10-22T00:57:23.029149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Reading the features and responder csv files to understand the dataset\n# Load the dataset\nfeatures_data = pd.read_csv('/kaggle/input/jane-street-real-time-market-data-forecasting/features.csv')\nresponders_data =  pd.read_csv('/kaggle/input/jane-street-real-time-market-data-forecasting/responders.csv')\n# Display the first few rows of the dataset\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T00:57:30.433820Z","iopub.execute_input":"2024-10-22T00:57:30.434228Z","iopub.status.idle":"2024-10-22T00:57:30.451498Z","shell.execute_reply.started":"2024-10-22T00:57:30.434197Z","shell.execute_reply":"2024-10-22T00:57:30.450808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get the length of the features_data and responders_data ###\nlen(features_data)\nlen(responders_data)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T00:57:34.313781Z","iopub.execute_input":"2024-10-22T00:57:34.314547Z","iopub.status.idle":"2024-10-22T00:57:34.321051Z","shell.execute_reply.started":"2024-10-22T00:57:34.314512Z","shell.execute_reply":"2024-10-22T00:57:34.320469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_data.head(10)\nresponders_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T07:26:54.807902Z","iopub.execute_input":"2024-10-21T07:26:54.808780Z","iopub.status.idle":"2024-10-21T07:26:54.834012Z","shell.execute_reply.started":"2024-10-21T07:26:54.808742Z","shell.execute_reply":"2024-10-21T07:26:54.832843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## The Problem solving for the dataset starts from here starting with installing the right packages ###\n######################################################################################################","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install polars","metadata":{"execution":{"iopub.status.busy":"2024-10-22T01:02:29.651619Z","iopub.execute_input":"2024-10-22T01:02:29.652228Z","iopub.status.idle":"2024-10-22T01:06:22.268089Z","shell.execute_reply.started":"2024-10-22T01:02:29.652196Z","shell.execute_reply":"2024-10-22T01:06:22.267072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\"\nimport pandas as pd\nimport polars as pl\ntrain = \\\npl.scan_parquet(\n    f\"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet\"\n).\\\nselect(\n    pl.int_range(pl.len(), dtype=pl.UInt64).alias(\"id\"),\n    pl.all(),\n).collect()\n\n# Read the parquet file and convert to pandas dataframe\n# Slice to only get the first 40,000 rows\ntrain_df = train.to_pandas().iloc[:950000]\n#In a similar manner i can read the test files\ntest = \\\npl.scan_parquet(\n    f\"/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet\"\n).\\\nselect(\n    pl.int_range(pl.len(), dtype=pl.UInt64).alias(\"id\"),\n    pl.all(),\n).collect()\ntest_df = test.to_pandas().iloc[:950000]\n\n\"\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-10-22T01:01:09.844620Z","iopub.execute_input":"2024-10-22T01:01:09.845001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##reading the training daatset by partitions :\nimport pandas as pd\ntrain_df = pd.read_parquet('/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=5/part-0.parquet')\ntrain_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T04:01:37.000441Z","iopub.execute_input":"2024-10-22T04:01:37.000825Z","iopub.status.idle":"2024-10-22T04:01:38.167288Z","shell.execute_reply.started":"2024-10-22T04:01:37.000796Z","shell.execute_reply":"2024-10-22T04:01:38.166378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets read the test dataframe which can be used to make predictions and it will help to understand how can i move forward\n#In a similar manner i can read the test files\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport pandas as pd\n\n# Define a schema that matches your dataset\nschema = pa.schema([\n    ('date_id', pa.int32()),  # Set date_id explicitly as int32\n    # Add other fields here, matching their expected types from the dataset\n])\n\n# Read the parquet file using the defined schema\ntry:\n    test_table = pq.read_table('/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet', schema=schema)\n    test_df = test_table.to_pandas()  # Convert to pandas DataFrame\nexcept pa.ArrowInvalid as e:\n    print(f\"Schema mismatch error: {e}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T04:01:58.760478Z","iopub.execute_input":"2024-10-22T04:01:58.760870Z","iopub.status.idle":"2024-10-22T04:01:58.775108Z","shell.execute_reply.started":"2024-10-22T04:01:58.760829Z","shell.execute_reply":"2024-10-22T04:01:58.774330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Try using fastparquet engine\n#!pip install fastparquet\ntest_df = pd.read_parquet('/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet', engine='fastparquet')","metadata":{"execution":{"iopub.status.busy":"2024-10-22T04:02:01.994935Z","iopub.execute_input":"2024-10-22T04:02:01.995290Z","iopub.status.idle":"2024-10-22T04:02:02.023474Z","shell.execute_reply.started":"2024-10-22T04:02:01.995260Z","shell.execute_reply":"2024-10-22T04:02:02.022693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.columns","metadata":{"execution":{"iopub.status.busy":"2024-10-22T04:02:04.361510Z","iopub.execute_input":"2024-10-22T04:02:04.362485Z","iopub.status.idle":"2024-10-22T04:02:04.367886Z","shell.execute_reply.started":"2024-10-22T04:02:04.362449Z","shell.execute_reply":"2024-10-22T04:02:04.367068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train_df))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T04:02:07.707406Z","iopub.execute_input":"2024-10-22T04:02:07.708362Z","iopub.status.idle":"2024-10-22T04:02:07.712604Z","shell.execute_reply.started":"2024-10-22T04:02:07.708325Z","shell.execute_reply":"2024-10-22T04:02:07.711788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test_df)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T04:02:10.199770Z","iopub.execute_input":"2024-10-22T04:02:10.200815Z","iopub.status.idle":"2024-10-22T04:02:10.205557Z","shell.execute_reply.started":"2024-10-22T04:02:10.200778Z","shell.execute_reply":"2024-10-22T04:02:10.204792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets understand the train_df \ntrain_df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-10-22T04:02:12.966927Z","iopub.execute_input":"2024-10-22T04:02:12.967923Z","iopub.status.idle":"2024-10-22T04:02:28.317346Z","shell.execute_reply.started":"2024-10-22T04:02:12.967887Z","shell.execute_reply":"2024-10-22T04:02:28.316523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##lets check the null values in the train df\ntrain_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-10-22T04:02:38.608171Z","iopub.execute_input":"2024-10-22T04:02:38.608546Z","iopub.status.idle":"2024-10-22T04:02:39.206488Z","shell.execute_reply.started":"2024-10-22T04:02:38.608518Z","shell.execute_reply":"2024-10-22T04:02:39.205714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets train a simple model on to understand how its working\nprint(train_df.columns)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T04:02:41.747501Z","iopub.execute_input":"2024-10-22T04:02:41.747838Z","iopub.status.idle":"2024-10-22T04:02:41.752737Z","shell.execute_reply.started":"2024-10-22T04:02:41.747810Z","shell.execute_reply":"2024-10-22T04:02:41.751889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nmissing_values = train_df.isnull().sum()\n\n# Filter columns that have missing values\nmissing_values = missing_values[missing_values > 0]\n\n# Create a bar plot of missing values\nplt.figure(figsize=(19, 6))\nmissing_values.plot(kind='bar', color='skyblue')\nplt.title('Missing Values per Column')\nplt.ylabel('Number of Missing Values')\nplt.xlabel('Columns')\nplt.xticks(rotation=90)\nplt.tight_layout()\n\n# Display the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-22T04:02:46.686563Z","iopub.execute_input":"2024-10-22T04:02:46.687401Z","iopub.status.idle":"2024-10-22T04:02:47.606667Z","shell.execute_reply.started":"2024-10-22T04:02:46.687367Z","shell.execute_reply":"2024-10-22T04:02:47.605897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_df.columns)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T03:55:44.314733Z","iopub.execute_input":"2024-10-22T03:55:44.315789Z","iopub.status.idle":"2024-10-22T03:55:44.320692Z","shell.execute_reply.started":"2024-10-22T03:55:44.315745Z","shell.execute_reply":"2024-10-22T03:55:44.319944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#let createa heatmap of correlations to understand the feature collinearity within the dataset :\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Drop all responder columns except responder_6\ndata = train_df.drop(columns=['responder_0', 'responder_1', 'responder_2', 'responder_3', \n                        'responder_4', 'responder_5', 'responder_7', 'responder_8'])\n\n# Set up the figure size for a large heatmap\nplt.figure(figsize=(20, 16))\n\n# Generate the heatmap for the dataset correlations\nsns.heatmap(data.corr(), annot=False, cmap='coolwarm', linewidths=0.5)\n\n# Add title\nplt.title('Heatmap of Correlations for All Columns (Including responder_6)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-22T03:55:47.455653Z","iopub.execute_input":"2024-10-22T03:55:47.456614Z","iopub.status.idle":"2024-10-22T03:57:36.430244Z","shell.execute_reply.started":"2024-10-22T03:55:47.456575Z","shell.execute_reply":"2024-10-22T03:57:36.429456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training a LGM Model for the training dataset to find out the feature importance by using the mode.This is an initial step in\n##learning to get the best fit model and make transition to deep learning model using tensorflow.  #########\n!pip install lightgbm","metadata":{"execution":{"iopub.status.busy":"2024-10-22T03:18:00.624244Z","iopub.execute_input":"2024-10-22T03:18:00.624617Z","iopub.status.idle":"2024-10-22T03:19:21.585974Z","shell.execute_reply.started":"2024-10-22T03:18:00.624586Z","shell.execute_reply":"2024-10-22T03:19:21.584806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Step 1: Drop columns with all NaN values\ntrain_df = train_df.dropna(axis=1, how='all')\n\n# Step 2: Split the data into features and target\nX = train_df.drop(columns=[ 'date_id', 'time_id',\\\n                           'feature_39','feature_40', 'feature_43',\\\n                           'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49',\\\n                           'feature_62', 'feature_63', 'feature_64','feature_65', 'feature_66',\\\n                           'responder_3','responder_8','responder_6','responder_7','responder_4','responder_5','responder_2',\\\n                           'feature_09','feature_06','feature_60','feature_07','feature_15', \\\n                           'feature_33','feature_34','feature_35','feature_36','feature_37','feature_38','feature_51',\\\n                           'feature_54','feature_56','feature_57','feature_59',\\\n                           'feature_61','feature_67','feature_68','feature_69','feature_70','feature_71','feature_72',\\\n                           'feature_75','feature_76','feature_77','feature_78','feature_05',\\\n                            'responder_0','responder_1', 'feature_00','feature_01','feature_02','feature_03','feature_04',\\\n                          'feature_21','feature_26','feature_27',\\\n                           'feature_31','feature_58','feature_50','feature_17','feature_16','feature_24','feature_12','feature_14','feature_41',\\\n                          'feature_32','feature_73'])  \n# Features\ny = train_df['responder_6']  # Target variable\nprint(X.columns)\n# Step 3: Handle missing values using SimpleImputer\nimputer = SimpleImputer(strategy='mean')  # Replace NaN values with the mean\nX = imputer.fit_transform(X)  # Apply imputer to the features\n\n# Step 4: Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Step 5: Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Step 6: Initialize and train the SGDRegressor model\nmodel = SGDRegressor(max_iter=1000, tol=1e-3, random_state=42)\n#model = LGBMRegressor(n_estimators=100, learning_rate=0.1, num_leaves=31)\nmodel.fit(X_train_scaled, y_train)\n\n# Step 7: Make predictions\ny_pred = model.predict(X_test_scaled)\n\n# Step 8: Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"Mean Squared Error: {mse}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T04:10:31.044583Z","iopub.execute_input":"2024-10-22T04:10:31.044982Z","iopub.status.idle":"2024-10-22T04:10:42.462462Z","shell.execute_reply.started":"2024-10-22T04:10:31.044947Z","shell.execute_reply":"2024-10-22T04:10:42.460755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets see the columsn present within the test_df \nprint(test_df.columns)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T04:08:22.304850Z","iopub.execute_input":"2024-10-22T04:08:22.305155Z","iopub.status.idle":"2024-10-22T04:08:22.312152Z","shell.execute_reply.started":"2024-10-22T04:08:22.305126Z","shell.execute_reply":"2024-10-22T04:08:22.310882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets prediction the submision.csv file using the above lgbm model and then we can improve further ###\nimport pandas as pd\n\nimport pandas as pd\n\n# Step 1: Drop unnecessary columns in the test dataset\n# Columns to drop in test_df based on the columns that were dropped from train_df\ncolumns_to_drop_in_test = ['is_scored','feature_34','feature_05','feature_35', 'feature_36', 'feature_37', 'feature_38',\n                           'feature_51', 'feature_54', 'feature_56', 'feature_57', 'feature_59',\n                           'feature_61', 'feature_67', 'feature_68', 'feature_69', 'feature_70', \n                           'feature_71', 'feature_72', 'feature_75', 'feature_76', 'feature_77',\n                           'feature_78','feature_00', 'feature_01', 'feature_02', 'feature_03','feature_04',\\\n                          'feature_06', 'feature_07', 'feature_09', 'feature_21','feature_21','feature_26','feature_27',\\\n                          'feature_31','feature_40','feature_43','feature_45','feature_46','feature_47',\\\n                          'feature_48','feature_49','feature_60','feature_62','feature_63','feature_64','feature_65',\\\n                          'feature_66', 'row_id', 'date_id', 'time_id','feature_15','feature_33','feature_39',\\\n                          'feature_00','feature_01','feature_02','feature_03','feature_04','feature_21','feature_26','feature_27',\\\n                           'feature_31','feature_58','feature_50','feature_17','feature_16','feature_24','feature_12','feature_14',\\\n                          'feature_41',\\\n                          'feature_32','feature_73']\n\n# Drop the columns from test_df that were not used in training\nX_test_df = test_df.drop(columns=columns_to_drop_in_test)\n\n# Step 2: Handle missing values in test_df using the same SimpleImputer from train_df\nX_test_df = imputer.transform(X_test_df)  # Use the same imputer fitted on the training data\n\n# Step 3: Standardize test_df using the same StandardScaler from train_df\nX_test_df_scaled = scaler.transform(X_test_df)  # Use the same scaler fitted on the training data\n\n# Step 4: Make predictions on the test_df using the trained model\ny_test_pred = model.predict(X_test_df_scaled)\n\n# Step 5: Create a DataFrame with 'id' column and the predictions\nsubmission_df = pd.DataFrame({\n    'row_id': test_df['row_id'],  # Use 'id' column from test_df as identifier\n    'prediction': y_test_pred\n})\n\n# Step 6: Save the DataFrame as a CSV file for submission to Kaggle\n#submission_df.to_csv('submission.csv', index=False)\n#print(submission_df)\n#print(\"Submission file 'submission.csv' created successfully!\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T04:12:50.065183Z","iopub.execute_input":"2024-10-22T04:12:50.066125Z","iopub.status.idle":"2024-10-22T04:12:50.078253Z","shell.execute_reply.started":"2024-10-22T04:12:50.066087Z","shell.execute_reply":"2024-10-22T04:12:50.077474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 9: Get feature importance (coefficients)\nimport matplotlib.pyplot as plt\ncoefficients = model.coef_\n\n# Step 10: Create a DataFrame to display feature importance\nfeatures_df = pd.DataFrame({\n    'Feature': train_df.drop(columns=[ 'date_id', 'time_id',\\\n                           'feature_39','feature_40', 'feature_43',\\\n                           'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49',\\\n                           'feature_62', 'feature_63', 'feature_64','feature_65', 'feature_66',\\\n                           'responder_3','responder_8','responder_6','responder_7','responder_4','responder_5','responder_2',\\\n                           'feature_09','feature_06','feature_60','feature_07','feature_15', \\\n                           'feature_33','feature_34','feature_35','feature_36','feature_37','feature_38','feature_51',\\\n                           'feature_54','feature_56','feature_57','feature_59',\\\n                           'feature_61','feature_67','feature_68','feature_69','feature_70','feature_71','feature_72',\\\n                           'feature_75','feature_76','feature_77','feature_78','feature_05',\\\n                            'responder_0','responder_1', 'feature_00','feature_01','feature_02','feature_03','feature_04',\\\n                          'feature_21','feature_26','feature_27',\\\n                           'feature_31','feature_58','feature_50','feature_17','feature_16','feature_24','feature_12','feature_14',\\\n                                     'feature_41',\\\n                          'feature_32','feature_73']).columns,\n    'Coefficient': coefficients\n})\n\n# Step 11: Sort by absolute coefficient value to get the most important features\nfeatures_df['Abs_Coefficient'] = features_df['Coefficient'].abs()\ntop_features = features_df.sort_values(by='Abs_Coefficient', ascending=False)\n\n# Display the sorted feature importance\nprint(top_features)\n\n# Step 12: Optionally, visualize the top 10 features\ntop_10_features = top_features.head(30)\n\nplt.figure(figsize=(10, 6))\nplt.barh(top_10_features['Feature'], top_10_features['Abs_Coefficient'], color='skyblue')\nplt.xlabel('Absolute Coefficient Value (Importance)')\nplt.ylabel('Feature')\nplt.title('Top 10 Most Important Features in SGDRegressor')\nplt.gca().invert_yaxis()  # Invert y-axis to display the most important feature at the top\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-22T04:11:11.291117Z","iopub.execute_input":"2024-10-22T04:11:11.291486Z","iopub.status.idle":"2024-10-22T04:11:11.666469Z","shell.execute_reply.started":"2024-10-22T04:11:11.291456Z","shell.execute_reply":"2024-10-22T04:11:11.665693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 6: Get and print the coefficients (these represent the model weights)\ncoefficients = model.coef_\n\n# Display the coefficients\nprint(\"Model Coefficients (Weights):\")\nfor feature, coef in zip(train_df.drop(columns=['date_id', 'time_id']).columns, coefficients):\n    print(f\"{feature}: {coef}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-22T04:11:21.388920Z","iopub.execute_input":"2024-10-22T04:11:21.389275Z","iopub.status.idle":"2024-10-22T04:11:22.062678Z","shell.execute_reply.started":"2024-10-22T04:11:21.389246Z","shell.execute_reply":"2024-10-22T04:11:22.061966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Training a tensorflow model to run it on GPU as well instead of just running on TPU as the above model is memory intensive \n##we need to more efficient model and this will definitely work .Tensorflow is generatiev AI framework. ######","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Finding the relationship of variables selected as features with the target variables ##\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}