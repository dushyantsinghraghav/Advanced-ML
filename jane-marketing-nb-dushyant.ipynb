{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.15","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84493,"databundleVersionId":9871156,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Modeling Financial Markets with Jane Street - Kaggle Challenge\n\nIn this notebook, we explore the challenges of modeling financial markets, where time series and distributions often deviate from traditional assumptions. Financial data can exhibit **fat-tailed distributions**, **non-stationary behavior**, and frequent structural shifts, making it a uniquely difficult environment for building predictive models.\n\nThe dataset provided for this challenge includes **79 anonymized features** and **9 responders**, representing real market data over time. The objective is to forecast **responder_6**, one of the responders, up to six months into the future.\n\nThrough this project, we aim to build machine learning models capable of handling the complexities of modern financial markets, using real-world data from Jane Street's trading strategies. The models developed here are designed to aid decision-making in an ever-evolving, human-driven market influenced by technological advances, economic trends, and geopolitical changes.\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-22T00:57:21.150204Z","iopub.execute_input":"2024-10-22T00:57:21.150599Z","iopub.status.idle":"2024-10-22T00:57:23.029822Z","shell.execute_reply.started":"2024-10-22T00:57:21.150566Z","shell.execute_reply":"2024-10-22T00:57:23.029149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Reading the features and responder csv files to understand the dataset\n# Load the dataset\nfeatures_data = pd.read_csv('/kaggle/input/jane-street-real-time-market-data-forecasting/features.csv')\nresponders_data =  pd.read_csv('/kaggle/input/jane-street-real-time-market-data-forecasting/responders.csv')\n# Display the first few rows of the dataset\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T00:57:30.433820Z","iopub.execute_input":"2024-10-22T00:57:30.434228Z","iopub.status.idle":"2024-10-22T00:57:30.451498Z","shell.execute_reply.started":"2024-10-22T00:57:30.434197Z","shell.execute_reply":"2024-10-22T00:57:30.450808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get the length of the features_data and responders_data ###\nlen(features_data)\nlen(responders_data)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T00:57:34.313781Z","iopub.execute_input":"2024-10-22T00:57:34.314547Z","iopub.status.idle":"2024-10-22T00:57:34.321051Z","shell.execute_reply.started":"2024-10-22T00:57:34.314512Z","shell.execute_reply":"2024-10-22T00:57:34.320469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_data.head(10)\nresponders_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T07:26:54.807902Z","iopub.execute_input":"2024-10-21T07:26:54.808780Z","iopub.status.idle":"2024-10-21T07:26:54.834012Z","shell.execute_reply.started":"2024-10-21T07:26:54.808742Z","shell.execute_reply":"2024-10-21T07:26:54.832843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exploratory Data Analysis (EDA)\n\nThe EDA involved data cleaning and visualization to better understand the dataset:\n\n1. **Data Cleaning**:\n   - Dropped columns with all missing values.\n   - Handled missing data using imputation (replacing with the mean).\n   - Created new features, like `multiplied_feature`, to enhance the model's predictive power.\n\n2. **Visualization**:\n   - A bar plot was used to visualize the number of missing values per column.\n   - A heatmap was generated to analyze correlations between features, aiding in understanding feature collinearity.\n   - Scatter plots and feature engineering were performed to explore relationships and improve model accuracy.\n","metadata":{}},{"cell_type":"code","source":"## The Problem solving for the dataset starts from here starting with installing the right packages ###\n######################################################################################################","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##reading the training daatset by partitions :\nimport pandas as pd\ntrain_df = pd.read_parquet('/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=5/part-0.parquet')\ntrain_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T12:08:17.790592Z","iopub.execute_input":"2024-10-22T12:08:17.790827Z","iopub.status.idle":"2024-10-22T12:08:30.085496Z","shell.execute_reply.started":"2024-10-22T12:08:17.790802Z","shell.execute_reply":"2024-10-22T12:08:30.084828Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"   date_id  time_id  symbol_id    weight  feature_00  feature_01  feature_02  \\\n0      850        0          0  2.087724   -0.276877   -2.385324   -1.086325   \n1      850        0          1  3.752097   -0.168178   -2.161023   -0.511679   \n2      850        0          2  1.225099   -0.520426   -1.718115   -0.817358   \n3      850        0          3  1.467042   -0.061985   -1.818735   -0.990254   \n4      850        0          5  3.144071   -0.321442   -1.964041   -0.409452   \n5      850        0          7  2.197618   -0.920352   -2.753816   -0.823805   \n6      850        0          8  2.094038    0.133232   -2.195577   -1.386584   \n7      850        0          9  1.474736    0.086528   -2.147897   -1.487129   \n8      850        0         10  0.892125   -0.258390   -2.537289   -0.438095   \n9      850        0         11  1.271155   -0.885654   -2.396034   -1.070651   \n\n   feature_03  feature_04  feature_05  ...  feature_78  responder_0  \\\n0    0.049463    3.427029   -4.671824  ...   -0.295312    -0.346724   \n1    0.192425    3.162096   -4.386098  ...   -0.444008    -0.086088   \n2   -0.270528    3.314825   -2.578923  ...   -0.267447     0.636380   \n3    0.274284    3.810929   -1.111770  ...   -0.157564     2.754708   \n4   -0.343893    3.069664   -2.929145  ...   -0.138634     0.151213   \n5   -0.022112    3.007740   -3.375129  ...   -0.177937    -0.964391   \n6    0.360822    3.671874   -2.466010  ...   -0.452496    -0.059468   \n7   -0.299004    3.660887   -5.913468  ...   -0.205526     0.132412   \n8   -0.205909    3.344720   -2.983913  ...    3.720120    -0.233305   \n9   -0.198664    3.690419   -3.567837  ...   -0.345609    -0.274564   \n\n   responder_1  responder_2  responder_3  responder_4  responder_5  \\\n0     0.054874    -0.050004     0.803933     0.725200     0.141694   \n1     0.056619    -0.396622     0.396958     1.208221    -1.016843   \n2     0.280909     1.168953    -2.395850    -1.057062    -3.009765   \n3     0.792327     0.403978    -0.096906    -0.263321     0.094231   \n4     0.207094     0.139006    -0.440090     0.145323    -0.967585   \n5    -0.683774    -1.064332    -0.717571    -0.712514    -1.853505   \n6     0.072518    -0.253422     0.191875     0.162306    -0.048974   \n7     0.152140    -0.058068    -0.037485     0.159067    -1.626778   \n8    -0.142806    -0.601231    -0.220472     0.361943    -0.950540   \n9    -0.155235    -0.761869    -0.438279     0.604514    -1.022208   \n\n   responder_6  responder_7  responder_8  \n0     1.461546     0.779843     0.404129  \n1     0.789595     1.251492    -1.701416  \n2    -2.848316    -0.974728    -5.000000  \n3    -0.749164    -0.691085    -0.089415  \n4    -0.882600     0.131690    -1.548052  \n5    -0.055907    -0.444895    -2.813932  \n6     0.336493     0.140711     0.091906  \n7    -0.311158    -0.018691    -3.114785  \n8     0.025254     0.430311    -1.193361  \n9    -0.379646     0.948544    -0.715813  \n\n[10 rows x 92 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date_id</th>\n      <th>time_id</th>\n      <th>symbol_id</th>\n      <th>weight</th>\n      <th>feature_00</th>\n      <th>feature_01</th>\n      <th>feature_02</th>\n      <th>feature_03</th>\n      <th>feature_04</th>\n      <th>feature_05</th>\n      <th>...</th>\n      <th>feature_78</th>\n      <th>responder_0</th>\n      <th>responder_1</th>\n      <th>responder_2</th>\n      <th>responder_3</th>\n      <th>responder_4</th>\n      <th>responder_5</th>\n      <th>responder_6</th>\n      <th>responder_7</th>\n      <th>responder_8</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>850</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2.087724</td>\n      <td>-0.276877</td>\n      <td>-2.385324</td>\n      <td>-1.086325</td>\n      <td>0.049463</td>\n      <td>3.427029</td>\n      <td>-4.671824</td>\n      <td>...</td>\n      <td>-0.295312</td>\n      <td>-0.346724</td>\n      <td>0.054874</td>\n      <td>-0.050004</td>\n      <td>0.803933</td>\n      <td>0.725200</td>\n      <td>0.141694</td>\n      <td>1.461546</td>\n      <td>0.779843</td>\n      <td>0.404129</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>850</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3.752097</td>\n      <td>-0.168178</td>\n      <td>-2.161023</td>\n      <td>-0.511679</td>\n      <td>0.192425</td>\n      <td>3.162096</td>\n      <td>-4.386098</td>\n      <td>...</td>\n      <td>-0.444008</td>\n      <td>-0.086088</td>\n      <td>0.056619</td>\n      <td>-0.396622</td>\n      <td>0.396958</td>\n      <td>1.208221</td>\n      <td>-1.016843</td>\n      <td>0.789595</td>\n      <td>1.251492</td>\n      <td>-1.701416</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>850</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1.225099</td>\n      <td>-0.520426</td>\n      <td>-1.718115</td>\n      <td>-0.817358</td>\n      <td>-0.270528</td>\n      <td>3.314825</td>\n      <td>-2.578923</td>\n      <td>...</td>\n      <td>-0.267447</td>\n      <td>0.636380</td>\n      <td>0.280909</td>\n      <td>1.168953</td>\n      <td>-2.395850</td>\n      <td>-1.057062</td>\n      <td>-3.009765</td>\n      <td>-2.848316</td>\n      <td>-0.974728</td>\n      <td>-5.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>850</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1.467042</td>\n      <td>-0.061985</td>\n      <td>-1.818735</td>\n      <td>-0.990254</td>\n      <td>0.274284</td>\n      <td>3.810929</td>\n      <td>-1.111770</td>\n      <td>...</td>\n      <td>-0.157564</td>\n      <td>2.754708</td>\n      <td>0.792327</td>\n      <td>0.403978</td>\n      <td>-0.096906</td>\n      <td>-0.263321</td>\n      <td>0.094231</td>\n      <td>-0.749164</td>\n      <td>-0.691085</td>\n      <td>-0.089415</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>850</td>\n      <td>0</td>\n      <td>5</td>\n      <td>3.144071</td>\n      <td>-0.321442</td>\n      <td>-1.964041</td>\n      <td>-0.409452</td>\n      <td>-0.343893</td>\n      <td>3.069664</td>\n      <td>-2.929145</td>\n      <td>...</td>\n      <td>-0.138634</td>\n      <td>0.151213</td>\n      <td>0.207094</td>\n      <td>0.139006</td>\n      <td>-0.440090</td>\n      <td>0.145323</td>\n      <td>-0.967585</td>\n      <td>-0.882600</td>\n      <td>0.131690</td>\n      <td>-1.548052</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>850</td>\n      <td>0</td>\n      <td>7</td>\n      <td>2.197618</td>\n      <td>-0.920352</td>\n      <td>-2.753816</td>\n      <td>-0.823805</td>\n      <td>-0.022112</td>\n      <td>3.007740</td>\n      <td>-3.375129</td>\n      <td>...</td>\n      <td>-0.177937</td>\n      <td>-0.964391</td>\n      <td>-0.683774</td>\n      <td>-1.064332</td>\n      <td>-0.717571</td>\n      <td>-0.712514</td>\n      <td>-1.853505</td>\n      <td>-0.055907</td>\n      <td>-0.444895</td>\n      <td>-2.813932</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>850</td>\n      <td>0</td>\n      <td>8</td>\n      <td>2.094038</td>\n      <td>0.133232</td>\n      <td>-2.195577</td>\n      <td>-1.386584</td>\n      <td>0.360822</td>\n      <td>3.671874</td>\n      <td>-2.466010</td>\n      <td>...</td>\n      <td>-0.452496</td>\n      <td>-0.059468</td>\n      <td>0.072518</td>\n      <td>-0.253422</td>\n      <td>0.191875</td>\n      <td>0.162306</td>\n      <td>-0.048974</td>\n      <td>0.336493</td>\n      <td>0.140711</td>\n      <td>0.091906</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>850</td>\n      <td>0</td>\n      <td>9</td>\n      <td>1.474736</td>\n      <td>0.086528</td>\n      <td>-2.147897</td>\n      <td>-1.487129</td>\n      <td>-0.299004</td>\n      <td>3.660887</td>\n      <td>-5.913468</td>\n      <td>...</td>\n      <td>-0.205526</td>\n      <td>0.132412</td>\n      <td>0.152140</td>\n      <td>-0.058068</td>\n      <td>-0.037485</td>\n      <td>0.159067</td>\n      <td>-1.626778</td>\n      <td>-0.311158</td>\n      <td>-0.018691</td>\n      <td>-3.114785</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>850</td>\n      <td>0</td>\n      <td>10</td>\n      <td>0.892125</td>\n      <td>-0.258390</td>\n      <td>-2.537289</td>\n      <td>-0.438095</td>\n      <td>-0.205909</td>\n      <td>3.344720</td>\n      <td>-2.983913</td>\n      <td>...</td>\n      <td>3.720120</td>\n      <td>-0.233305</td>\n      <td>-0.142806</td>\n      <td>-0.601231</td>\n      <td>-0.220472</td>\n      <td>0.361943</td>\n      <td>-0.950540</td>\n      <td>0.025254</td>\n      <td>0.430311</td>\n      <td>-1.193361</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>850</td>\n      <td>0</td>\n      <td>11</td>\n      <td>1.271155</td>\n      <td>-0.885654</td>\n      <td>-2.396034</td>\n      <td>-1.070651</td>\n      <td>-0.198664</td>\n      <td>3.690419</td>\n      <td>-3.567837</td>\n      <td>...</td>\n      <td>-0.345609</td>\n      <td>-0.274564</td>\n      <td>-0.155235</td>\n      <td>-0.761869</td>\n      <td>-0.438279</td>\n      <td>0.604514</td>\n      <td>-1.022208</td>\n      <td>-0.379646</td>\n      <td>0.948544</td>\n      <td>-0.715813</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 92 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_df = pd.read_parquet('/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet/date_id=0/part-0.parquet')\ntest_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T12:10:15.126942Z","iopub.execute_input":"2024-10-22T12:10:15.127402Z","iopub.status.idle":"2024-10-22T12:10:15.167754Z","shell.execute_reply.started":"2024-10-22T12:10:15.127370Z","shell.execute_reply":"2024-10-22T12:10:15.167105Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   row_id  date_id  time_id  symbol_id    weight  is_scored  feature_00  \\\n0       0        0        0          0  3.169998       True         0.0   \n1       1        0        0          1  2.165993       True         0.0   \n2       2        0        0          2  3.065550       True         0.0   \n3       3        0        0          3  2.698642       True         0.0   \n4       4        0        0          4  1.803330       True         0.0   \n5       5        0        0          5  2.605776       True         0.0   \n6       6        0        0          6  1.047993       True         0.0   \n7       7        0        0          7  4.231289       True         0.0   \n8       8        0        0          8  2.600524       True         0.0   \n9       9        0        0          9  1.256275       True         0.0   \n\n   feature_01  feature_02  feature_03  ...  feature_69  feature_70  \\\n0         0.0         0.0         0.0  ...        -0.0        -0.0   \n1        -0.0         0.0         0.0  ...        -0.0        -0.0   \n2        -0.0         0.0         0.0  ...         0.0        -0.0   \n3         0.0         0.0         0.0  ...         0.0        -0.0   \n4        -0.0         0.0         0.0  ...        -0.0        -0.0   \n5        -0.0         0.0         0.0  ...        -0.0        -0.0   \n6        -0.0         0.0         0.0  ...        -0.0        -0.0   \n7         0.0         0.0         0.0  ...         0.0        -0.0   \n8         0.0         0.0         0.0  ...        -0.0        -0.0   \n9        -0.0         0.0         0.0  ...        -0.0        -0.0   \n\n   feature_71  feature_72  feature_73  feature_74  feature_75  feature_76  \\\n0         0.0         0.0         NaN         NaN         0.0         0.0   \n1         0.0        -0.0         NaN         NaN         0.0         0.0   \n2         0.0         0.0         NaN         NaN         0.0         0.0   \n3         0.0         0.0         NaN         NaN         0.0         0.0   \n4         0.0        -0.0         NaN         NaN         0.0         0.0   \n5         0.0         0.0         NaN         NaN         0.0         0.0   \n6         0.0         0.0         NaN         NaN         0.0         0.0   \n7         0.0        -0.0         NaN         NaN         0.0         0.0   \n8         0.0         0.0         NaN         NaN         0.0         0.0   \n9         0.0        -0.0         NaN         NaN         0.0         0.0   \n\n   feature_77  feature_78  \n0        -0.0        -0.0  \n1         0.0         0.0  \n2        -0.0        -0.0  \n3        -0.0        -0.0  \n4         0.0         0.0  \n5         0.0         0.0  \n6         0.0         0.0  \n7        -0.0        -0.0  \n8         0.0         0.0  \n9        -0.0        -0.0  \n\n[10 rows x 85 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>date_id</th>\n      <th>time_id</th>\n      <th>symbol_id</th>\n      <th>weight</th>\n      <th>is_scored</th>\n      <th>feature_00</th>\n      <th>feature_01</th>\n      <th>feature_02</th>\n      <th>feature_03</th>\n      <th>...</th>\n      <th>feature_69</th>\n      <th>feature_70</th>\n      <th>feature_71</th>\n      <th>feature_72</th>\n      <th>feature_73</th>\n      <th>feature_74</th>\n      <th>feature_75</th>\n      <th>feature_76</th>\n      <th>feature_77</th>\n      <th>feature_78</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3.169998</td>\n      <td>True</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2.165993</td>\n      <td>True</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>3.065550</td>\n      <td>True</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2.698642</td>\n      <td>True</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1.803330</td>\n      <td>True</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2.605776</td>\n      <td>True</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n      <td>1.047993</td>\n      <td>True</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7</td>\n      <td>4.231289</td>\n      <td>True</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>2.600524</td>\n      <td>True</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>1.256275</td>\n      <td>True</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.0</td>\n      <td>-0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 85 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_df.columns","metadata":{"execution":{"iopub.status.busy":"2024-10-22T12:10:18.458117Z","iopub.execute_input":"2024-10-22T12:10:18.458653Z","iopub.status.idle":"2024-10-22T12:10:18.464386Z","shell.execute_reply.started":"2024-10-22T12:10:18.458614Z","shell.execute_reply":"2024-10-22T12:10:18.463638Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"Index(['row_id', 'date_id', 'time_id', 'symbol_id', 'weight', 'is_scored',\n       'feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04',\n       'feature_05', 'feature_06', 'feature_07', 'feature_08', 'feature_09',\n       'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14',\n       'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19',\n       'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24',\n       'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29',\n       'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34',\n       'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39',\n       'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44',\n       'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49',\n       'feature_50', 'feature_51', 'feature_52', 'feature_53', 'feature_54',\n       'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59',\n       'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64',\n       'feature_65', 'feature_66', 'feature_67', 'feature_68', 'feature_69',\n       'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74',\n       'feature_75', 'feature_76', 'feature_77', 'feature_78'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"print(len(train_df))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T04:24:11.448864Z","iopub.execute_input":"2024-10-22T04:24:11.449172Z","iopub.status.idle":"2024-10-22T04:24:11.453015Z","shell.execute_reply.started":"2024-10-22T04:24:11.449145Z","shell.execute_reply":"2024-10-22T04:24:11.452239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test_df)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T04:24:14.879891Z","iopub.execute_input":"2024-10-22T04:24:14.880211Z","iopub.status.idle":"2024-10-22T04:24:14.885101Z","shell.execute_reply.started":"2024-10-22T04:24:14.880184Z","shell.execute_reply":"2024-10-22T04:24:14.884358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets understand the train_df \ntrain_df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-10-22T04:24:17.073906Z","iopub.execute_input":"2024-10-22T04:24:17.074252Z","iopub.status.idle":"2024-10-22T04:24:33.217608Z","shell.execute_reply.started":"2024-10-22T04:24:17.074221Z","shell.execute_reply":"2024-10-22T04:24:33.216959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##lets check the null values in the train df\ntrain_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-10-22T04:02:38.608171Z","iopub.execute_input":"2024-10-22T04:02:38.608546Z","iopub.status.idle":"2024-10-22T04:02:39.206488Z","shell.execute_reply.started":"2024-10-22T04:02:38.608518Z","shell.execute_reply":"2024-10-22T04:02:39.205714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets train a simple model on to understand how its working\nprint(train_df.columns)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T04:02:41.747501Z","iopub.execute_input":"2024-10-22T04:02:41.747838Z","iopub.status.idle":"2024-10-22T04:02:41.752737Z","shell.execute_reply.started":"2024-10-22T04:02:41.747810Z","shell.execute_reply":"2024-10-22T04:02:41.751889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Summary of Code\n\nThe code block visualizes the missing values in the dataset `train_df` using a bar plot:\n\n1. **Identifying Missing Values**: \n   - The `isnull().sum()` function is used to count the number of missing values in each column of the dataframe `train_df`.\n   - It then filters out the columns where the count of missing values is greater than zero.\n\n2. **Plotting Missing Values**:\n   - A bar plot is generated using Matplotlib to visualize the number of missing values for each column that contains any.\n   - The plot is customized with a title, labels for the x and y axes, and the x-axis tick labels are rotated by 90 degrees for better readability.\n\n3. **Figure Size**: The plot size is set to 19x6 inches for optimal viewing.\n\n4. **Plot Display**: Finally, the layout is adjusted using `tight_layout()` to avoid overlapping elements, and the plot is displayed using `plt.show()`.\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nmissing_values = train_df.isnull().sum()\n\n# Filter columns that have missing values\nmissing_values = missing_values[missing_values > 0]\n\n# Create a bar plot of missing values\nplt.figure(figsize=(19, 6))\nmissing_values.plot(kind='bar', color='skyblue')\nplt.title('Missing Values per Column')\nplt.ylabel('Number of Missing Values')\nplt.xlabel('Columns')\nplt.xticks(rotation=90)\nplt.tight_layout()\n\n# Display the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-22T04:02:46.686563Z","iopub.execute_input":"2024-10-22T04:02:46.687401Z","iopub.status.idle":"2024-10-22T04:02:47.606667Z","shell.execute_reply.started":"2024-10-22T04:02:46.687367Z","shell.execute_reply":"2024-10-22T04:02:47.605897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_df.columns)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T03:55:44.314733Z","iopub.execute_input":"2024-10-22T03:55:44.315789Z","iopub.status.idle":"2024-10-22T03:55:44.320692Z","shell.execute_reply.started":"2024-10-22T03:55:44.315745Z","shell.execute_reply":"2024-10-22T03:55:44.319944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Summary of Code\n\nThe code block generates a heatmap to visualize the correlations between features in the dataset `train_df`, excluding all `responder` columns except `responder_6`:\n\n1. **Dropping Responder Columns**:\n   - All `responder` columns, except `responder_6`, are removed from `train_df` to focus on the remaining features for correlation analysis.\n\n2. **Heatmap Setup**:\n   - The plot size is set to 20x16 inches for a large and detailed heatmap visualization.\n\n3. **Generating the Heatmap**:\n   - Seaborn's `heatmap()` function is used to display the correlation matrix of the remaining columns.\n   - The color scheme chosen is `coolwarm`, and `linewidths=0.5` adds gridlines between cells for better visual separation.\n\n4. **Plot Display**:\n   - A title is added to indicate the purpose of the heatmap, and the plot is displayed using `plt.show()`.\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#let createa heatmap of correlations to understand the feature collinearity within the dataset :\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Drop all responder columns except responder_6\ndata = train_df.drop(columns=['responder_0', 'responder_1', 'responder_2', 'responder_3', \n                        'responder_4', 'responder_5', 'responder_7', 'responder_8'])\n\n# Set up the figure size for a large heatmap\nplt.figure(figsize=(20, 16))\n\n# Generate the heatmap for the dataset correlations\nsns.heatmap(data.corr(), annot=False, cmap='coolwarm', linewidths=0.5)\n\n# Add title\nplt.title('Heatmap of Correlations for All Columns (Including responder_6)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-22T03:55:47.455653Z","iopub.execute_input":"2024-10-22T03:55:47.456614Z","iopub.status.idle":"2024-10-22T03:57:36.430244Z","shell.execute_reply.started":"2024-10-22T03:55:47.456575Z","shell.execute_reply":"2024-10-22T03:57:36.429456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training a LGM Model for the training dataset to find out the feature importance by using the mode.This is an initial step in\n##learning to get the best fit model and make transition to deep learning model using tensorflow.  #########\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T03:18:00.624244Z","iopub.execute_input":"2024-10-22T03:18:00.624617Z","iopub.status.idle":"2024-10-22T03:19:21.585974Z","shell.execute_reply.started":"2024-10-22T03:18:00.624586Z","shell.execute_reply":"2024-10-22T03:19:21.584806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Summary of Code\n\nThis code implements a basic regression pipeline using the `SGDRegressor` model to predict the target variable `responder_6`:\n\n1. **Data Preprocessing**:\n   - Drop columns with all missing values.\n   - Split the dataset into features (`X`) and target variable (`y`), dropping a specific set of columns.\n\n2. **Handling Missing Values**:\n   - Missing values in `X` are replaced with the column mean using `SimpleImputer`.\n\n3. **Train-Test Split**:\n   - The data is split into training and testing sets, with 30% of the data used for testing.\n\n4. **Feature Scaling**:\n   - Standardize the features using `StandardScaler` to ensure they are on the same scale.\n\n5. **Model Training**:\n   - The `SGDRegressor` is initialized and trained on the scaled training data.\n\n6. **Predictions and Evaluation**:\n   - The model makes predictions on the test set, and the performance is evaluated using Mean Squared Error (MSE).\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Step 1: Drop columns with all NaN values\ntrain_df = train_df.dropna(axis=1, how='all')\n\n# Step 2: Split the data into features and target\nX = train_df.drop(columns=[ 'date_id', 'time_id',\\\n                           'feature_39','feature_40', 'feature_43',\\\n                           'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49',\\\n                           'feature_62', 'feature_63', 'feature_64','feature_65', 'feature_66',\\\n                           'responder_3','responder_8','responder_6','responder_7','responder_4','responder_5','responder_2',\\\n                           'feature_09','feature_06','feature_60','feature_07','feature_15', \\\n                           'feature_33','feature_34','feature_35','feature_36','feature_37','feature_38','feature_51',\\\n                           'feature_54','feature_56','feature_57','feature_59',\\\n                           'feature_61','feature_67','feature_68','feature_69','feature_70','feature_71','feature_72',\\\n                           'feature_75','feature_76','feature_77','feature_78','feature_05',\\\n                            'responder_0','responder_1', 'feature_00','feature_01','feature_02','feature_03','feature_04',\\\n                          'feature_21','feature_26','feature_27',\\\n                           'feature_31','feature_58','feature_50','feature_17','feature_16','feature_24','feature_12','feature_14','feature_41',\\\n                          'feature_32','feature_73'])  \n# Features\ny = train_df['responder_6']  # Target variable\nprint(X.columns)\n# Step 3: Handle missing values using SimpleImputer\nimputer = SimpleImputer(strategy='mean')  # Replace NaN values with the mean\nX = imputer.fit_transform(X)  # Apply imputer to the features\n\n# Step 4: Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Step 5: Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Step 6: Initialize and train the SGDRegressor model\nmodel = SGDRegressor(max_iter=1000, tol=1e-3, random_state=42)\n#model = LGBMRegressor(n_estimators=100, learning_rate=0.1, num_leaves=31)\nmodel.fit(X_train_scaled, y_train)\n\n# Step 7: Make predictions\ny_pred = model.predict(X_test_scaled)\n\n# Step 8: Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"Mean Squared Error: {mse}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T12:10:25.297635Z","iopub.execute_input":"2024-10-22T12:10:25.298142Z","iopub.status.idle":"2024-10-22T12:10:38.822137Z","shell.execute_reply.started":"2024-10-22T12:10:25.298109Z","shell.execute_reply":"2024-10-22T12:10:38.820382Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Index(['symbol_id', 'weight', 'feature_08', 'feature_10', 'feature_11',\n       'feature_13', 'feature_18', 'feature_19', 'feature_20', 'feature_22',\n       'feature_23', 'feature_25', 'feature_28', 'feature_29', 'feature_30',\n       'feature_42', 'feature_44', 'feature_52', 'feature_53', 'feature_55',\n       'feature_74'],\n      dtype='object')\nMean Squared Error: 0.8137859714357146\n","output_type":"stream"}]},{"cell_type":"code","source":"#lets see the columsn present within the test_df \nprint(test_df.columns)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T12:10:38.836499Z","iopub.execute_input":"2024-10-22T12:10:38.837033Z","iopub.status.idle":"2024-10-22T12:10:38.853541Z","shell.execute_reply.started":"2024-10-22T12:10:38.836970Z","shell.execute_reply":"2024-10-22T12:10:38.852145Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Index(['row_id', 'date_id', 'time_id', 'symbol_id', 'weight', 'is_scored',\n       'feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04',\n       'feature_05', 'feature_06', 'feature_07', 'feature_08', 'feature_09',\n       'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14',\n       'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19',\n       'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24',\n       'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29',\n       'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34',\n       'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39',\n       'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44',\n       'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49',\n       'feature_50', 'feature_51', 'feature_52', 'feature_53', 'feature_54',\n       'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59',\n       'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64',\n       'feature_65', 'feature_66', 'feature_67', 'feature_68', 'feature_69',\n       'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74',\n       'feature_75', 'feature_76', 'feature_77', 'feature_78'],\n      dtype='object')\n","output_type":"stream"}]},{"cell_type":"code","source":"#lets prediction the submision.csv file using the above lgbm model and then we can improve further ###\nimport pandas as pd\n\nimport pandas as pd\n\n# Step 1: Drop unnecessary columns in the test dataset\n# Columns to drop in test_df based on the columns that were dropped from train_df\ncolumns_to_drop_in_test = ['is_scored','feature_34','feature_05','feature_35', 'feature_36', 'feature_37', 'feature_38',\n                           'feature_51', 'feature_54', 'feature_56', 'feature_57', 'feature_59',\n                           'feature_61', 'feature_67', 'feature_68', 'feature_69', 'feature_70', \n                           'feature_71', 'feature_72', 'feature_75', 'feature_76', 'feature_77',\n                           'feature_78','feature_00', 'feature_01', 'feature_02', 'feature_03','feature_04',\\\n                          'feature_06', 'feature_07', 'feature_09', 'feature_21','feature_21','feature_26','feature_27',\\\n                          'feature_31','feature_40','feature_43','feature_45','feature_46','feature_47',\\\n                          'feature_48','feature_49','feature_60','feature_62','feature_63','feature_64','feature_65',\\\n                          'feature_66', 'row_id', 'date_id', 'time_id','feature_15','feature_33','feature_39',\\\n                          'feature_00','feature_01','feature_02','feature_03','feature_04','feature_21','feature_26','feature_27',\\\n                           'feature_31','feature_58','feature_50','feature_17','feature_16','feature_24','feature_12','feature_14',\\\n                          'feature_41',\\\n                          'feature_32','feature_73']\n\n# Drop the columns from test_df that were not used in training\nX_test_df = test_df.drop(columns=columns_to_drop_in_test)\n\n# Step 2: Handle missing values in test_df using the same SimpleImputer from train_df\nX_test_df = imputer.transform(X_test_df)  # Use the same imputer fitted on the training data\n\n# Step 3: Standardize test_df using the same StandardScaler from train_df\nX_test_df_scaled = scaler.transform(X_test_df)  # Use the same scaler fitted on the training data\n\n# Step 4: Make predictions on the test_df using the trained model\ny_test_pred = model.predict(X_test_df_scaled)\n\n# Step 5: Create a DataFrame with 'id' column and the predictions\nsubmission_df = pd.DataFrame({\n    'row_id': test_df['row_id'],  # Use 'id' column from test_df as identifier\n    'prediction': y_test_pred\n})\n\n# Step 6: Save the DataFrame as a CSV file for submission to Kaggle\n#submission_df.to_csv('submissions.csv', index=False)\n#print(submission_df)\nsubmission_df.to_parquet('submission.parquet', engine='pyarrow', index=False)\nprint(\"Submission file 'submissions.parquet' created successfully!\")\n#submission_check = pd.read_parquet('submissions.parquet')\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T12:11:00.749832Z","iopub.execute_input":"2024-10-22T12:11:00.750569Z","iopub.status.idle":"2024-10-22T12:11:00.763515Z","shell.execute_reply.started":"2024-10-22T12:11:00.750532Z","shell.execute_reply":"2024-10-22T12:11:00.762703Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Submission file 'submissions.parquet' created successfully!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Feature Importance: Top 15 Features\n\nTo highlight the most impactful features, the coefficients of the top 15 features from the trained `SGDRegressor` model were extracted and visualized:\n\n1. **Extracting Top 15 Coefficients**:\n   - The absolute values of the feature coefficients were calculated and the top 15 most influential features were selected.\n\n2. **Visualization**:\n   - A bar plot was generated to display the absolute values of the top 15 feature coefficients, illustrating their importance in the model's predictions.\n","metadata":{}},{"cell_type":"code","source":"# Step 9: Get feature importance (coefficients)\nimport matplotlib.pyplot as plt\ncoefficients = model.coef_\n\n# Step 10: Create a DataFrame to display feature importance\nfeatures_df = pd.DataFrame({\n    'Feature': train_df.drop(columns=[ 'date_id', 'time_id',\\\n                           'feature_39','feature_40', 'feature_43',\\\n                           'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49',\\\n                           'feature_62', 'feature_63', 'feature_64','feature_65', 'feature_66',\\\n                           'responder_3','responder_8','responder_6','responder_7','responder_4','responder_5','responder_2',\\\n                           'feature_09','feature_06','feature_60','feature_07','feature_15', \\\n                           'feature_33','feature_34','feature_35','feature_36','feature_37','feature_38','feature_51',\\\n                           'feature_54','feature_56','feature_57','feature_59',\\\n                           'feature_61','feature_67','feature_68','feature_69','feature_70','feature_71','feature_72',\\\n                           'feature_75','feature_76','feature_77','feature_78','feature_05',\\\n                            'responder_0','responder_1', 'feature_00','feature_01','feature_02','feature_03','feature_04',\\\n                          'feature_21','feature_26','feature_27',\\\n                           'feature_31','feature_58','feature_50','feature_17','feature_16','feature_24','feature_12','feature_14',\\\n                                     'feature_41',\\\n                          'feature_32','feature_73']).columns,\n    'Coefficient': coefficients\n})\n\n# Step 11: Sort by absolute coefficient value to get the most important features\nfeatures_df['Abs_Coefficient'] = features_df['Coefficient'].abs()\ntop_features = features_df.sort_values(by='Abs_Coefficient', ascending=False)\n\n# Display the sorted feature importance\nprint(top_features)\n\n# Step 12: Optionally, visualize the top 10 features\ntop_10_features = top_features.head(30)\n\nplt.figure(figsize=(10, 6))\nplt.barh(top_10_features['Feature'], top_10_features['Abs_Coefficient'], color='skyblue')\nplt.xlabel('Absolute Coefficient Value (Importance)')\nplt.ylabel('Feature')\nplt.title('Top 10 Most Important Features in SGDRegressor')\nplt.gca().invert_yaxis()  # Invert y-axis to display the most important feature at the top\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-22T04:11:11.291117Z","iopub.execute_input":"2024-10-22T04:11:11.291486Z","iopub.status.idle":"2024-10-22T04:11:11.666469Z","shell.execute_reply.started":"2024-10-22T04:11:11.291456Z","shell.execute_reply":"2024-10-22T04:11:11.665693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 6: Get and print the coefficients (these represent the model weights)\ncoefficients = model.coef_\n\n# Display the coefficients\nprint(\"Model Coefficients (Weights):\")\nfor feature, coef in zip(train_df.drop(columns=['date_id', 'time_id']).columns, coefficients):\n    print(f\"{feature}: {coef}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-22T04:11:21.388920Z","iopub.execute_input":"2024-10-22T04:11:21.389275Z","iopub.status.idle":"2024-10-22T04:11:22.062678Z","shell.execute_reply.started":"2024-10-22T04:11:21.389246Z","shell.execute_reply":"2024-10-22T04:11:22.061966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Training a tensorflow model to run it on GPU as well instead of just running on TPU as the above model is memory intensive \n##we need to more efficient model and this will definitely work .Tensorflow is generatiev AI framework. ######","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Finding the relationship of variables selected as features with the target variable ##\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Sample the data to reduce plot time\nsampled_df = train_df.sample(frac=0.3, random_state=42)  # Use 10% of the data\n\n# List of features\nfeatures = ['symbol_id', 'weight', 'feature_08', 'feature_10', 'feature_11',\n            'feature_13', 'feature_18', 'feature_19', 'feature_20', 'feature_22',\n            'feature_23', 'feature_25', 'feature_28', 'feature_29', 'feature_30',\n            'feature_42', 'feature_44', 'feature_52', 'feature_53', 'feature_55', \n            'feature_74']\n\n# Define the number of rows and columns for subplots\nn_cols = 4  # 4 plots per row\nn_rows = (len(features) + n_cols - 1) // n_cols  # Calculate the number of rows\n\n# Create subplots\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 15))\naxes = axes.flatten()  # Flatten the axes array for easy indexing\n\n# Plot each feature against the target using the sampled data\nfor i, feature in enumerate(features):\n    sns.scatterplot(x=sampled_df[feature], y=sampled_df['responder_6'], ax=axes[i])\n    axes[i].set_title(f'Relationship between {feature} and responder_6')\n    axes[i].set_xlabel(feature)\n    axes[i].set_ylabel('responder_6')\n\n# Remove any unused subplots\nfor j in range(i + 1, len(axes)):\n    fig.delaxes(axes[j])\n\nplt.tight_layout()  # Adjust subplots to fit into the figure area\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T04:38:45.924545Z","iopub.execute_input":"2024-10-22T04:38:45.924959Z","iopub.status.idle":"2024-10-22T04:39:59.318600Z","shell.execute_reply.started":"2024-10-22T04:38:45.924926Z","shell.execute_reply":"2024-10-22T04:39:59.317569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the correlation of each feature with the target 'responder_6'\ncorrelations = train_df[features + ['responder_6']].corr()['responder_6'].sort_values(ascending=False)\n\n# Display the correlation values\nprint(correlations)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T04:40:50.476381Z","iopub.execute_input":"2024-10-22T04:40:50.477314Z","iopub.status.idle":"2024-10-22T04:40:57.286217Z","shell.execute_reply.started":"2024-10-22T04:40:50.477262Z","shell.execute_reply":"2024-10-22T04:40:57.285180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"### Summary of Code\n\nThis code performs additional feature engineering to potentially improve the model's accuracy by creating a new feature and updating the feature set:\n\n1. **Creating a New Feature**:\n   - A new feature, `multiplied_feature`, is created by multiplying `feature_10`, `feature_11`, `symbol_id`, and `feature_52`.\n\n2. **Feature Update**:\n   - The feature set is updated to include the newly created `multiplied_feature` along with existing relevant features.\n\n3. **Model Preparation**:\n   - The updated feature matrix (`X`) and target variable (`y`) are prepared for further model training and evaluation.\n","metadata":{}},{"cell_type":"code","source":"##lets do some more feature engineering to increase the acuuracy of the model and see its effect aftere analysing the scatter plots :\n# Create a new feature by multiplying 'feature_10', 'feature_11', and 'symbol_id'\ntrain_df['multiplied_feature'] = train_df['feature_10'] * train_df['feature_11'] * train_df['symbol_id'] * train_df['feature_52'] \n# Display the new feature\nprint(train_df[['feature_10', 'feature_11', 'symbol_id', 'multiplied_feature']].head())\n\n# Update the list of features to include the new feature\nfeatures = ['symbol_id', 'weight', 'feature_08', 'feature_10', 'feature_11',\n            'feature_13', 'feature_18', 'feature_19', 'feature_20', 'feature_22',\n            'feature_23', 'feature_25', 'feature_28', 'feature_29', 'feature_30',\n            'feature_42', 'feature_44', 'feature_52', 'feature_53', 'feature_55', \n            'feature_74', 'multiplied_feature']\n\n# Prepare the feature matrix (X) and the target (y)\nX = train_df[features]\ny = train_df['responder_6']\n\n# Proceed with training the model as before","metadata":{"execution":{"iopub.status.busy":"2024-10-22T04:49:55.951029Z","iopub.execute_input":"2024-10-22T04:49:55.952078Z","iopub.status.idle":"2024-10-22T04:49:56.163592Z","shell.execute_reply.started":"2024-10-22T04:49:55.952043Z","shell.execute_reply":"2024-10-22T04:49:56.162694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 3: Handle missing values using SimpleImputer\nimputer = SimpleImputer(strategy='mean')  # Replace NaN values with the mean\nX = imputer.fit_transform(X)  # Apply imputer to the features\n\n# Step 4: Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Step 5: Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Step 6: Initialize and train the SGDRegressor model\nmodel_2 = SGDRegressor(max_iter=1000, tol=1e-3, random_state=42)\n#model = LGBMRegressor(n_estimators=100, learning_rate=0.1, num_leaves=31)\nmodel_2.fit(X_train_scaled, y_train)\n\n# Step 7: Make predictions\ny_pred_second = model_2.predict(X_test_scaled)\n\n# Step 8: Evaluate the model\nmse = mean_squared_error(y_test, y_pred_second)\nprint(f\"Mean Squared Error: {mse}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T04:49:59.117008Z","iopub.execute_input":"2024-10-22T04:49:59.117534Z","iopub.status.idle":"2024-10-22T04:50:09.788247Z","shell.execute_reply.started":"2024-10-22T04:49:59.117501Z","shell.execute_reply":"2024-10-22T04:50:09.786805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The accuracy of  the model hasn't changed and hence we need to try other feature engineering methods ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}